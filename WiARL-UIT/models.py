# -*- coding: utf-8 -*-
"""Models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ur2DPoPP-q6rJAIK67KtxObxzKayYGW-

# Environment preparations
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os
from six.moves import urllib
import tempfile
import glob
import os.path
import sys

import numpy as np
import pandas as pd
import tensorflow as tf

# Examine software versions
print(__import__('sys').version)
print(tf.__version__)
#print(tf.keras.__version__)

# Utilizing GPU for training
physical_devices = tf.config.experimental.list_physical_devices('GPU')
print("Num GPUs Available: ", len(physical_devices))
if (len(physical_devices) > 0):
    tf.config.experimental.set_memory_growth(physical_devices[0], True)

tf.keras.backend.clear_session()

"""## Loading dataset"""

external_path = '/content/drive/MyDrive/NEW_DTS/new_sangtrai_mid'
!ls $external_path # Checking directory

# Creating list of filenames
filenames = glob.glob(external_path + "/*.npy")
filenames_p = [i.split('/')[-1] for i in filenames]

# Copying data to local environment for quicker loading to numpy
#!rm -rf ./csi_data8
!cp -r $external_path ./csi_data

# Creating empty lists to store data and labels
all_data = []
all_labels = []

# Reading data from numpy files and adding to empty list
for f in filenames_p:
  if f == 'a_500.npy': continue # Minimal file bug
  data_in = np.load('./csi_data' + '/' + f)
  # print(f'{f}: {data_in.shape}') # for checking file shapes
  all_data.append(data_in)
  all_labels.append(f.split('_')[0])

# Converting list of arrays to numpy array
data_in = np.array(all_data)
label_in = np.array(all_labels)

# Converting labels to numbers
label_in[label_in == 'a'] = 0
label_in[label_in == 'b'] = 1
label_in[label_in == 'c'] = 2
label_in[label_in == 'd'] = 3
label_in[label_in == 'e'] = 4
label_in[label_in == 'f'] = 5

# Converting label array to number type
label_in = label_in.astype(np.float64)
label_in

"""## Preparing dataset"""

from sklearn.model_selection import train_test_split
# Splitting train/test sets with 80:20 ratio and commonly used random state 42
train_data, test_data, train_label, test_label = train_test_split(data_in, label_in, test_size=0.20, random_state=42)

print(f'{train_data.shape}, {train_label.shape}')
print(f'{test_data.shape}, {test_label.shape}')

"""## Plotting function"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import itertools
import matplotlib.pyplot as plt

def plot_confusion_matrix(cm, classes,
                        normalize=False,
                        title='Confusion matrix',
                        cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
            horizontalalignment="center",
            color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

a = np.load('/content/drive/MyDrive/NEW_DTS/new_sangtrai_mid/a_24.npy')
plt.plot(a)
#plt.figure(4)
#plt.plot(h)
#plt.figure(5)
#plt.plot(i)
#plt.show()

"""# Apply data to deep learning models

## MLP Model
"""

def get_MLP_model(input_dim, learning_rate, num_classes):
    InputLayer = tf.keras.layers.InputLayer
    Dense = tf.keras.layers.Dense
    Flatten = tf.keras.layers.Flatten

    model = tf.keras.Sequential(
      [
          InputLayer(input_shape=input_dim),
          Flatten(),
          Dense(1024, activation=tf.nn.relu),
          Dense(128, activation=tf.nn.relu),
          Dense(num_classes, activation=tf.nn.softmax)
      ])
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=learning_rate)

    model.compile(
        loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

MLP_model = get_MLP_model(train_data[0].shape, 0.0001, len(np.unique(train_label)))
MLP_model.summary()

# Creating checkpoint callback for loading best weights
checkpoint_filepath = './MLP_checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

MLPt = MLP_model.fit(train_data, train_label, batch_size=128, epochs=100, validation_split=0.2, shuffle=True, callbacks=[model_checkpoint_callback])

predictions = MLP_model.predict(
      x=test_data
    , batch_size=1
    , verbose=2
)

# Rounding predictions values to map into confusion matrix
rounded_predictions = np.argmax(predictions, axis=-1)

cm_plot_labels = ['a', 'b','c', 'd', 'e', 'f']
cm = confusion_matrix(y_true=test_label, y_pred=rounded_predictions)
accuracy = accuracy_score(test_label, rounded_predictions) * 100
precision = precision_score(test_label, rounded_predictions, average = 'macro', zero_division = 1) * 100
recall = recall_score(test_label, rounded_predictions, average = 'macro', zero_division = 1) * 100
f1 = f1_score(test_label, rounded_predictions, average = 'macro', zero_division = 1) * 100
plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title=f'Confusion Matrix: MLP. Accuracy {accuracy:.02f}%')
#print("Per class: ", cm.diagonal()/cm.sum(axis=1))
print("Accuracy: ", accuracy)
print("Precision: ", precision)
print("Recall: ", recall)

"""## CNN Model"""

def get_CNN_model(input_dim, learning_rate, num_classes):
    InputLayer = tf.keras.layers.InputLayer
    Conv2D = tf.keras.layers.Conv2D
    Dense = tf.keras.layers.Dense
    Flatten = tf.keras.layers.Flatten
    MaxPool2D = tf.keras.layers.MaxPool2D

    model = tf.keras.Sequential(
      [
          InputLayer(input_shape=input_dim),
          Conv2D(32, kernel_size=7, strides=1, activation='relu'),
          MaxPool2D(pool_size=2),
          Conv2D(64, kernel_size=5, strides=2, padding='same', activation='relu'),
          MaxPool2D(pool_size=2),
          Conv2D(96, kernel_size=3, strides=1, activation='relu'),
          MaxPool2D(pool_size=2),
          Flatten(),
          Dense(128, activation=tf.nn.relu),
          Dense(num_classes, activation=tf.nn.softmax)
      ])
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=learning_rate)

    model.compile(
        loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# Expanding dataset dimensions since CNN2D takes in 3 dimensions: HWC
train_data2 = train_data.copy()
train_data2 = np.expand_dims(train_data2, axis=-1)
train_data2.shape

CNN_model = get_CNN_model(train_data2[0].shape, 0.001, len(np.unique(train_label)))
CNN_model.summary()

CNN_checkpoint_filepath = './CNN_checkpoint'
CNN_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=CNN_checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

Cnn = CNN_model.fit(train_data2, train_label, batch_size=128, epochs=100, validation_split=0.2, shuffle=True, callbacks=[CNN_checkpoint_callback])

predictions_2 = CNN_model.predict(
      x=test_data
    , batch_size=1
    , verbose=2
)

# Rounding predictions values to map into confusion matrix
rounded_predictions_2 = np.argmax(predictions_2, axis=-1)

cm_plot_labels_2 = ['a', 'b', 'c', 'd', 'e', 'f']
cm2 = confusion_matrix(y_true=test_label, y_pred=rounded_predictions_2)
accuracy_2 = accuracy_score(test_label, rounded_predictions_2) * 100
precision_2 = precision_score(test_label, rounded_predictions_2, average = "macro", zero_division = 1) * 100
recall_2 = recall_score(test_label, rounded_predictions_2, average = "macro", zero_division = 1) * 100
f1_2 = f1_score(test_label, rounded_predictions_2, average = "macro", zero_division = 1) * 100
plot_confusion_matrix(cm=cm2, classes=cm_plot_labels_2, title=f'Confusion Matrix: CNN. Accuracy {accuracy_2:.02f}%')
print("Per class: ", cm2.diagonal()/cm2.sum(axis=1))
print("Accuracy: ", accuracy_2)
print("Precision: ", precision_2)
print("Recall: ", recall_2)
print("F1: ", f1_2)

"""# **Learning Curves**"""

plt.figure(figsize=(12,10))
plt.subplot(2,2,1)
plt.plot(Cnn.history['accuracy'])
plt.plot(Cnn.history['val_accuracy'])
plt.ylim(0, 1.1)
plt.title('model accuracy: CNN')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'valid'], loc='lower right')
plt.subplot(2,2,2)
plt.plot(Cnn.history['loss'])
plt.plot(Cnn.history['val_loss'])
plt.title('model loss: CNN')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'valid'], loc='upper right')
plt.ylim([-0.1,2])


plt.subplot(2,2,3)
plt.plot(MLPt.history['accuracy'])
plt.plot(MLPt.history['val_accuracy'])
plt.ylim(0, 1.1)
plt.title('model accuracy: MLP')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'valid'], loc='lower right')
plt.subplot(2,2,4)
plt.plot(MLPt.history['loss'])
plt.plot(MLPt.history['val_loss'])
plt.title('model loss: MLP')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'valid'], loc='upper right')
plt.ylim([-0.1,3])
plt.show()